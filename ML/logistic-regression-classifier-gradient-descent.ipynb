{"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**** I've tried to implement the logistic regression classifier with gradient descent from scratch. I'm new to Machine Learning, therefore any inputs or suggestions would be very helpful for me.**","metadata":{"_cell_guid":"6e8b7ef8-79cd-3132-a050-03d1fd9dbda6"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"496469ec-d3aa-0b5f-df1c-992283364cc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/Iris.csv')\n\ndf.head()","metadata":{"_cell_guid":"f61089a3-426f-a031-d698-c7a0b00d9671"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract sepal length and sepal width of setosa and versicolor for our binary calssification problem\nX = df.iloc[0:100, [1, 2]].values\n\ny = df.iloc[0:100, 5].values\n# set output lable value to 1 if it is setosa and 0 if versicolor.\ny = np.where(y == 'Iris-setosa', 1, 0)","metadata":{"_cell_guid":"333be199-ef20-d258-770d-2ccc664060b1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features standerdization\nX_std = np.copy(X)\n\nX_std[:,0] = (X_std[:,0] - X_std[:,0].mean()) / X_std[:,0].std()\nX_std[:,1] = (X_std[:,1] - X_std[:,1].mean()) / X_std[:,1].std()","metadata":{"_cell_guid":"54f3da65-ff60-8943-9826-8f3ad350c74e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression Cost Function\n\n![enter image description here][1]\n\n  [1]: https://i.stack.imgur.com/2uGpX.png","metadata":{"_cell_guid":"cfb2adba-11a1-e961-0b9c-d04e68c8e8ee"}},{"cell_type":"code","source":"# Define Logistic Regression hypothesis or sigmoid function\n\ndef sigmoid(X, theta):\n    \n    z = np.dot(X, theta[1:]) + theta[0]\n    \n    return 1.0 / ( 1.0 + np.exp(-z))","metadata":{"_cell_guid":"42675478-94bb-168e-4cec-e23192c0a1f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Logistic Regression Cost Function\ndef lrCostFunction(y, hx):\n  \n    # compute cost for given theta parameters\n    j = -y.dot(np.log(hx)) - ((1 - y).dot(np.log(1-hx)))\n    \n    return j","metadata":{"_cell_guid":"f9d737a9-5b8c-4a99-742b-4111e4d2ccc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Descent function to minimize the Logistic Regression Cost Function.\ndef lrGradient(X, y, theta, alpha, num_iter):\n    # empty list to store the value of the cost function over number of iterations\n    cost = []\n    \n    for i in range(num_iter):\n        # call sigmoid function \n        hx = sigmoid(X, theta)\n        # calculate error\n        error = hx - y\n        # calculate gradient\n        grad = X.T.dot(error)\n        # update values in theta\n        theta[0] = theta[0] - alpha * error.sum()\n        theta[1:] = theta[1:] - alpha * grad\n        \n        cost.append(lrCostFunction(y, hx))\n        \n    return cost        \n        ","metadata":{"_cell_guid":"c96464e8-aa47-a934-fb13-2a452d54fa11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# m = Number of training examples\n# n = number of features\nm, n = X.shape\n\n# initialize theta(weights) parameters to zeros\ntheta = np.zeros(1+n)\n\n# set learning rate to 0.01 and number of iterations to 500\nalpha = 0.01\nnum_iter = 500\n\ncost = lrGradient(X_std, y, theta, alpha, num_iter)","metadata":{"_cell_guid":"14d64d3f-9d9e-7c69-58ca-da0a237937a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a plot with number of iterations on the x-axis and the cost function on y-axis\nplt.plot(range(1, len(cost) + 1), cost)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.title('Logistic Regression')","metadata":{"_cell_guid":"e6873e23-ef33-de7d-0e33-6d8fadf1e3d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print theta paramters \nprint ('\\n Logisitc Regression bias(intercept) term :', theta[0])\nprint ('\\n Logisitc Regression estimated coefficients :', theta[1:])","metadata":{"_cell_guid":"986837ce-a646-1c93-453f-37c30f2ad179"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to predict the output label using the parameters\ndef lrPredict(X):\n    \n    return np.where(sigmoid(X,theta) >= 0.5, 1, 0)","metadata":{"_cell_guid":"dd53c954-70a2-44f9-3650-4b6221c4295d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_boundry(X, y, classifier, h=0.02):\n    # h = step size in the mesh\n  \n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n                         np.arange(x2_min, x2_max, h))\n    Z = classifier(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n  ","metadata":{"_cell_guid":"8c183a24-319a-36e8-9c00-db4c81204307"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decision_boundry(X_std, y, classifier=lrPredict)\nplt.title('Standardized Logistic Regression - Gradient Descent')\nplt.xlabel('sepal length ')\nplt.ylabel('sepal width ')\nplt.legend(loc='upper left')\nplt.tight_layout()","metadata":{"_cell_guid":"990808cd-750b-7744-bd22-3a9fd5433c37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\n\nlogreg = linear_model.LogisticRegression()\n\nlogreg.fit(X_std, y)\n\n# print theta paramters \nprint ('\\n sklearn bias(intercept) term :', logreg.intercept_)\nprint ('\\n sklearn estimated coefficients :', logreg.coef_)","metadata":{"_cell_guid":"f34dc51c-be6a-419b-588c-90465cb2c30d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_decision_boundry(X_std, y, classifier=logreg.predict)\nplt.title('Scikit  learn Logistic Regression Classifier')\nplt.xlabel('sepal length ')\nplt.ylabel('sepal width ')\nplt.legend(loc='upper left')\nplt.tight_layout()","metadata":{"_cell_guid":"2da841e0-1005-aa06-155b-02c6ab6d5904"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_cell_guid":"dc4de1d4-5f2c-57b3-4386-d6ca888dee6c"},"execution_count":null,"outputs":[]}]}